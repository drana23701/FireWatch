{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 days FWI Reanalysis forecast using 4 days input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do 10 day FWI-Reanalysis forecast using 4-days input of Temperature, Relative humidity, Total precipitation, and Wind speed. The sample data must span for at least 13 consecutive days. The file format of the stored data should be in netCDF4 format. A pretrained model checkpoint conrresponding to the input and forecast timespan will be required as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing working directory to import modules naturally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../src')\n",
    "from train import str2num, get_hparams, get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing installed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functionality\n",
    "import random\n",
    "from glob import glob\n",
    "from argparse import Namespace\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "# Keep the execution uncomplicated\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ploting purposes\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "# Helper modules\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensuring reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2334\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Data\n",
    "The next cell downloads the required data for performing inference using 2 (4) days weather forcings for 1 (10) days FWI prediction. The data is stored in `./deepfwi-mini-sample/fwi-forcings` and `./deepfwi-mini-sample/fwi-reanalysis`.\n",
    "\n",
    "If you already have the data downloaded, comment the cell below and please place it in the directory structure explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../examples')\n",
    "os.chdir('../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating pickled list of test-set files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few files...\n",
      "/Users/Dhruv/wildfire-forecasting/examples/epoch_99_100.ckpt\n",
      "/Users/Dhruv/wildfire-forecasting/examples/epoch_99_100.ckpt\n",
      "/Users/Dhruv/wildfire-forecasting/examples/epoch_99_100.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Only the files needed for output variable are specified.\n",
    "# Input varaible files for corresponding dates are auto-deduced.\n",
    "test_out_files = [f'/nvme0/fwi-reanalysis/ECMWF_FWI_201904{x:02}_1200_hr_fwi_e5.nc' for x in range(1, 14)]\n",
    "print(\"First few files...\", *test_out_files[:3], sep='\\n')\n",
    "\n",
    "# Pickling the list which can be used later\n",
    "with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "    pickle.dump(test_out_files, tmp)\n",
    "    test_set_path = tmp.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting the knobs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Storing hyperparamters and configuration in a dictionary\n",
    "hparams_dict = get_hparams(\n",
    "    # Feature density of U-Net layers\n",
    "    init_features=16,\n",
    "    # Number of input channels (=4*input days)\n",
    "    in_days=4,\n",
    "    # Number of prediction channels (=output days)\n",
    "    out_days=10,\n",
    "    # Loss metric\n",
    "    loss='mse',\n",
    "    # Batch size\n",
    "    batch_size=1,\n",
    "    # Number of GPUs to use\n",
    "    gpus=1,\n",
    "    # Turn on temporal and spatial constraints for case-study in Australia\n",
    "    case_study=False,\n",
    "    # Whether to ignore fire-prone regions\n",
    "    clip_fwi=False,\n",
    "    # Pickled list of output files in test-set\n",
    "    test_set=test_set_path,\n",
    "    # Model architecture\n",
    "    model='unet_tapered',\n",
    "    # Output dataloader\n",
    "    out='fwi_reanalysis',\n",
    "    # Directory with FWI Forcings input\n",
    "    forcings_dir='/nvme1/fwi-forcings',\n",
    "    # Directory with FWI Renanlysis output\n",
    "    reanalysis_dir='/nvme0/fwi-reanalysis',\n",
    "    # Custom mask stored as numpy array\n",
    "    mask='dataloader/mask.npy',\n",
    "    # Model checkpoint file used to load the pretrained weights\n",
    "    checkpoint_file='/w/deepfwi/src/model/checkpoints/pre_trained/5/4_10/epoch_75_82.ckpt'\n",
    "    )\n",
    "\n",
    "# Converting the dictionary to Namespace for easier access\n",
    "hparams = Namespace(**hparams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Setting the flag for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.eval = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'am'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qd/v4mk0cf97rg_d28qjwh17n000000gp/T/ipykernel_29482/3477995774.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the model architecture and attach with the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the pretrained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wildfire-forecasting/src/train.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wildfire-forecasting/src/model/base_model.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self, ModelDataset, force)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mModelDataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforcings_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforcings_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreanalysis_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreanalysis_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrp_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wildfire-forecasting/src/dataloader/fwi_reanalysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, out_var, out_mean, forecast_dir, forcings_dir, reanalysis_dir, transform, hparams, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 }\n\u001b[1;32m    106\u001b[0m                 test_inp = sum(\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0minp_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_out\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mget_out_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 )\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wildfire-forecasting/src/dataloader/fwi_reanalysis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 }\n\u001b[1;32m    106\u001b[0m                 test_inp = sum(\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0minp_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_out\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mget_out_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 )\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wildfire-forecasting/src/dataloader/fwi_reanalysis.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     87\u001b[0m         )\n\u001b[1;32m     88\u001b[0m         get_out_time = (\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         )\n\u001b[1;32m     91\u001b[0m         out_files = sorted(\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'am'"
     ]
    }
   ],
   "source": [
    "# Create the model architecture and attach with the data\n",
    "model = get_model(hparams)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model.load_state_dict(torch.load(hparams.checkpoint_file)[\"state_dict\"])\n",
    "\n",
    "# Turn off the gradients\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qd/v4mk0cf97rg_d28qjwh17n000000gp/T/ipykernel_29482/1928873941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Input tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Ground truth tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Input tensor\n",
    "x = model.data[0][0].unsqueeze(0)\n",
    "\n",
    "# Ground truth tensor\n",
    "y = model.data[0][1].unsqueeze(0)\n",
    "\n",
    "# Predicted tensor\n",
    "y_hat = model(x).detach()\n",
    "\n",
    "# Masking the prediction as done with the ground truth\n",
    "y_hat[torch.isnan(y)] = torch.tensor(float('nan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20, hparams.in_days*4))\n",
    "fig.suptitle('Input FWI-variables from 2019-04-01 till 2019-04-04', fontsize=25)\n",
    "for i in range(hparams.in_days):\n",
    "    for j in range(4):\n",
    "        ax = fig.add_subplot(hparams.in_days, 4, 4*i+j+1)\n",
    "        if i==0:\n",
    "            if j==0:\n",
    "                ax.set_title('Relative Humidity',fontsize='23', pad=20)\n",
    "            elif j==1:\n",
    "                ax.set_title('Temperature',fontsize='23', pad=20)\n",
    "            elif j==2:\n",
    "                ax.set_title('Precipitation',fontsize='23', pad=20)\n",
    "            else:\n",
    "                ax.set_title('Wind speed',fontsize='23', pad=20)\n",
    "        if j==0:\n",
    "            ax.set_ylabel(f'2019-04-{i+1:02}', fontsize='23', labelpad=20)\n",
    "        if j==1:\n",
    "            plt.imshow(x.squeeze()[4*i+j], cmap='gist_ncar')\n",
    "        elif j==2:\n",
    "            plt.imshow(x.squeeze()[4*i+j], cmap='flag')\n",
    "        else:\n",
    "            plt.imshow(x.squeeze()[4*i+j], cmap='hsv')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Helper function to add colorbar in the plots.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(im, title):\n",
    "    fig, ax = plt.subplots(figsize = (20,10))\n",
    "    fig.suptitle(title, fontsize=25)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('bottom', size='5%', pad=0.5)\n",
    "    im = ax.imshow(im, cmap='jet')\n",
    "    fig.colorbar(im, cax=cax, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot(y.squeeze()[i], f'Observed FWI-reanalysis for 2019-04-{4+i:02}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot(y_hat.squeeze()[i], f'Predicted FWI-reanalysis for 2019-04-{4+i:02}\\nAccuracy: \\\n",
    "    {((y-y_hat).squeeze()[i].abs()<9.4)[model.data.mask].float().mean()*100:.2f}%*')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>*Using half of MAD as the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot((y-y_hat).abs().squeeze()[i], f\"Error in predicted FWI-reanalysis for 2019-04-{4+i:02}\\nMAE: \\\n",
    "    {((y-y_hat).squeeze()[i].abs())[model.data.mask].float().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The receptive field size for input stays the same as for global prediction. After getting the global predictions we extract only the values falling within the case-study region coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_case = y[0][:, 355:480, 400:550]\n",
    "y_hat_case = y_hat[0][:, 355:480, 400:550]\n",
    "mask_case = model.data.mask[355:480, 400:550]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot(y_case.squeeze()[i], f'Observed FWI-reanalysis in the case-study region for 2019-04-{4+i:02}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot(y_hat_case.squeeze()[i], f'Predicted FWI-reanalysis in the case-study region for 2019-04-{4+i:02}\\nAccuracy: \\\n",
    "    {((y_case-y_hat_case).squeeze()[i].abs()<9.4)[mask_case].float().mean()*100:.2f}%*')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>*Using half of MAD as the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot((y_case-y_hat_case).abs().squeeze()[i], f\"Error in predicted FWI-reanalysis for 2019-04-{4+i:02}\\nMAE: \\\n",
    "    {((y_case-y_hat_case).squeeze()[i].abs())[mask_case].float().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trained with Box-Cox transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do inference on the model trained with Box-Cox transformed output variable, we would need the corresponding lambda value for the inverse transformation. The chekpoint used here requires the lambda = 0.1182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.boxcox = 0.1182\n",
    "hparams.checkpoint_file='/w/deepfwi/src/model/checkpoints/pre_trained/7/4_10/epoch_99_100.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model architecture and attach with the data\n",
    "model = get_model(hparams)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model.load_state_dict(torch.load(hparams.checkpoint_file)[\"state_dict\"])\n",
    "\n",
    "# Turn off the gradients\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tensor\n",
    "x = model.data[0][0].unsqueeze(0)\n",
    "\n",
    "# Ground truth tensor\n",
    "y = model.data[0][1].unsqueeze(0)\n",
    "\n",
    "# Predicted tensor\n",
    "y_hat = model(x).detach()\n",
    "y_hat = torch.from_numpy(\n",
    "    inv_boxcox(y_hat.cpu().numpy(), model.hparams.boxcox)\n",
    ")\n",
    "\n",
    "# Masking the prediction as done with the ground truth\n",
    "y_hat[torch.isnan(y)] = torch.tensor(float('nan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot(y_hat.squeeze()[i], f'Predicted FWI-reanalysis for 2019-04-{4+i:02}\\nAccuracy: \\\n",
    "    {((y-y_hat).squeeze()[i].abs()<9.4)[model.data.mask].float().mean()*100:.2f}%*')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>*Using half of MAD as the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot((y-y_hat).abs().squeeze()[i], f\"Error in predicted FWI-reanalysis for 2019-04-{4+i:02}\\nMAE: \\\n",
    "    {((y-y_hat).squeeze()[i].abs())[model.data.mask].float().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of passing inputs manually to the model, the Trainer can be used. It will run the prepared model over the entire data and generate the result metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trainer object responsible for running the model\n",
    "trainer = pl.Trainer(gpus=hparams.gpus)\n",
    "\n",
    "# Running inference with the supplied model\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did 10 days global forecast using 4-day input of Relative humidty, Temperature, Total precipitation, and Wind speed. We also looked at the error distribution of the prediction across the various regions and timescales.\n",
    "\n",
    "| Day | MAE | Accuracy | MSE |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0 | 3.798 | 88.19% | 44.332 |\n",
    "| 1 | 4.875 | 83.87% | 81.655 |\n",
    "| 2 | 5.042 | 82.79% | 77.965 |\n",
    "| 3 | 4.846 | 83.28% | 72.909 |\n",
    "| 4 | 5.134 | 82.41% | 79.106 |\n",
    "| 5 | 5.310 | 81.93% | 88.908 |\n",
    "| 6 | 5.584 | 81.70% | 123.03 |\n",
    "| 7 | 5.305 | 81.27% | 87.120 |\n",
    "| 8 | 5.596 | 80.84% | 98.665 |\n",
    "| 9 | 5.932 | 80.00% | 114.54 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
